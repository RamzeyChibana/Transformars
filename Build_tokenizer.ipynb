{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\python\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as vocab\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,infos = tfds.load('ted_hrlr_translate/pt_to_en',with_info=True,as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data , val_data = data[\"train\"],data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "but what if it were active ?\n"
     ]
    }
   ],
   "source": [
    "for pt,en in train_data.batch(1).take(2):\n",
    "    for example in pt.numpy() :\n",
    "        print(example.decode(\"utf-8\"))\n",
    "    for example in en.numpy():\n",
    "        print(example.decode(\"utf-8\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "train_en = train_data.map(lambda pt,en : en)\n",
    "train_pt = train_data.map(lambda pt,en : pt)\n",
    "\n",
    "for en in train_en.take(3) :\n",
    "    print(en.numpy().decode(\"utf-8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_file(path,vocab):\n",
    "    file = open(path,\"w\",encoding=\"utf-8\")\n",
    "    for token in vocab :\n",
    "        print(token,file=file)\n",
    "    file.close()\n",
    "    \n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens = [\"[PAD]\",\"[UNK]\",\"[START]\",\"[END]\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = vocab.bert_vocab_from_dataset(train_en.batch(1000).prefetch(tf.data.AUTOTUNE),8000,reserved_tokens=reserved_tokens,bert_tokenizer_params=bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['different', 'around', 'thank', 'say', 'day', 'good', 'her', 'through', 'today', 'same']\n",
      "there is 7010 token in english\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[0:10])\n",
    "print(en_vocab[200:210])\n",
    "print(f\"there is {len(en_vocab)} token in english\")\n",
    "\n",
    "\n",
    "vocab_to_file(\"tokens/en_tokens.txt\",en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_vocab = vocab.bert_vocab_from_dataset(train_pt.batch(1000).prefetch(tf.data.AUTOTUNE),8000,reserved_tokens=reserved_tokens,bert_tokenizer_params=bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['verdade', 'falar', 'todas', 'vou', 'portanto', 'pela', 'sem', 'aos', 'realmente', 'vezes']\n",
      "there is 7765 token in portuguese\n"
     ]
    }
   ],
   "source": [
    "print(pt_vocab[0:10])\n",
    "print(pt_vocab[200:210])\n",
    "print(f\"there is {len(pt_vocab)} token in portuguese\")\n",
    "\n",
    "vocab_to_file(\"tokens/pt_tokens.txt\",pt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
      " b'but what if it were active ?']\n",
      "[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n",
      "[87, 90, 107, 76, 129, 1852, 30]\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = text.BertTokenizer(\"tokens/en_tokens.txt\")\n",
    "for pt, en in train_data.batch(2).take(1):\n",
    "    print(en.numpy())\n",
    "    token_batch = en_tokenizer.tokenize(en)\n",
    "    token_batch = token_batch.merge_dims(-2,-1)\n",
    "    for ex in token_batch.to_list():\n",
    "        print(ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[2, 4006, 80, 54, 2199, 2302, 240, 3]]>\n",
      "<tf.RaggedTensor [[b'[START]', b'hello', b'is', b'ramzey', b'[END]']]>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_start_end(token_batch):\n",
    "    length = token_batch.bounding_shape()[0] #batch size\n",
    "    start=tf.cast(tf.fill([length,1],reserved_tokens.index(\"[START]\")),dtype=tf.int64)\n",
    "    end=tf.cast(tf.fill([length,1],reserved_tokens.index(\"[END]\")),dtype=tf.int64)\n",
    "    return tf.concat([start,token_batch,end],axis=1)\n",
    "\n",
    "token_batch = en_tokenizer.tokenize(\"hello is ramzey\")\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "tokens = add_start_end(token_batch)\n",
    "print(tokens)\n",
    "words = en_tokenizer.detokenize(tokens)\n",
    "print(words)\n",
    "def remove_reserved(token_text,reserved_tokens):\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    reg_ex = \"|\".join(bad_tokens)\n",
    "    bad_cells = tf.strings.regex_full_match(token_text, reg_ex)\n",
    "    good_cells = tf.logical_not(bad_cells)\n",
    "    result = tf.ragged.boolean_mask(token_text,good_cells)\n",
    "    return result\n",
    "\n",
    "#print(remove_reserved(words,reserved_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(tf.Module):\n",
    "    def __init__(self,vocab_path,reserved_tokens):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path)\n",
    "        self._reserved_tokens=reserved_tokens\n",
    "        self.vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "        f = open(vocab_path,\"r\",encoding=\"utf-8\")\n",
    "        tokens=f.read()\n",
    "        f.close()\n",
    "        self.vocab = tf.Variable(tokens.splitlines())\n",
    "        \n",
    "\n",
    "        \n",
    "        self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "        \n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "    @tf.function\n",
    "    def tokenize(self,strings):\n",
    "        encode = self.tokenizer.tokenize(strings)\n",
    "        encode = encode.merge_dims(-2,-1)\n",
    "        return add_start_end(encode)\n",
    "    @tf.function\n",
    "    def detokenize(self,encoded) :\n",
    "        decode = self.tokenizer.detokenize(encoded)\n",
    "        return remove_reserved(decode,self._reserved_tokens)\n",
    "    @tf.function\n",
    "    def lookup(self,tokens_ids):\n",
    "        words = tf.gather(self.vocab,tokens_ids)\n",
    "        return words\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab.shape[0]\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return self._reserved_tokens\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self.vocab_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizers\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizers\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizers = tf.Module()\n",
    "tokenizers.pt = MyTokenizer('tokens/pt_tokens.txt',reserved_tokens)\n",
    "tokenizers.en = MyTokenizer('tokens/en_tokens.txt',reserved_tokens)\n",
    "\n",
    "tf.saved_model.save(tokenizers,\"tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7765, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(\"tokenizers\")\n",
    "print(loaded_model.pt.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : hello my name is tensor\n",
      "encoded to:  <tf.RaggedTensor [[2, 4006, 99, 571, 80, 2358, 687, 3]]>\n",
      "encoded words:  <tf.RaggedTensor [[b'[START]', b'hello', b'my', b'name', b'is', b'tens', b'##or', b'[END]']]>\n",
      "decoded to:  [['[UNK]', 'hello', 'my', 'name', 'is', 'tensor']]\n"
     ]
    }
   ],
   "source": [
    "words = \"hello my name is tensor\"\n",
    "print(\"text :\",words)\n",
    "tokens=loaded_model.en.tokenize([words])\n",
    "tf.print(\"encoded to: \",tokens)\n",
    "tf.print(\"encoded words: \",loaded_model.en.lookup(tokens))\n",
    "tf.print(\"decoded to: \",loaded_model.en.detokenize(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
